{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 4529,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3321
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Text to Image Using Stable Diffusion",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepak-Acharya/Machine-leaning-Basic-/blob/Machine-Learning-Projects/Text_to_Image_Using_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'stable-diffusion-xl/pytorch/base-1-0/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3321%2F4529%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240302%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240302T191559Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2816c9f057a72da2ff6835bbae0631cb608f79c1b6e184057244761c8ae0f5bb33ded7a94ef1ec257cabebd7d7461b9b29d134f2e1a26bc1a6b3f086f07c367dead953a3a44264ac9bf2770985f41c0c47404567b15d5e879e385286fde0ab031414ed44e3aac47938e5d76949693c252a6214de6af3087ac5b59e24e410109342b657a8e27690b15e36659ff07be358fc42cc9abe413b3794ab384c4ead61756a48ab598ae00b81266a1eae9a88d53453d912c30628dadfc1a636ced80c898352c5986048afcdb8ff1b3873d28874adf6ab6b284afff762e6ba51265a26107a3bdf6f08f663c0062065f496760a78f1f09869a99855b91ceec3c24c1a73427f'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "XMAexrggmEkK"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-02T18:23:34.413481Z",
          "iopub.execute_input": "2024-03-02T18:23:34.413988Z",
          "iopub.status.idle": "2024-03-02T18:23:34.432802Z",
          "shell.execute_reply.started": "2024-03-02T18:23:34.413943Z",
          "shell.execute_reply": "2024-03-02T18:23:34.431646Z"
        },
        "trusted": true,
        "id": "ezoZMORumEkL",
        "outputId": "8bc0a9fc-534c-40c0-e661-1bb9ed62003c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/LICENSE.md\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/model_index.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/pipeline.png\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/README.md\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/sd_xl_base_1.0.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/sd_xl_offset_example-lora_1.0.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/01.png\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/comparison.png\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/sd_xl_base_1.0_0.9vae.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/.gitattributes\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/tokenizer_2/merges.txt\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/tokenizer_2/vocab.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/tokenizer_2/tokenizer_config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/tokenizer_2/special_tokens_map.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/tokenizer/merges.txt\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/tokenizer/vocab.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/tokenizer/tokenizer_config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/tokenizer/special_tokens_map.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/scheduler/scheduler_config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae/config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae/diffusion_pytorch_model.fp16.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae/diffusion_pytorch_model.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder_2/openvino_model.bin\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder_2/model.onnx\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder_2/config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder_2/openvino_model.xml\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder_2/model.fp16.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder_2/model.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder_2/model.onnx_data\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder/openvino_model.bin\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder/model.onnx\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder/config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder/openvino_model.xml\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder/model.fp16.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/text_encoder/model.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_encoder/openvino_model.bin\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_encoder/model.onnx\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_encoder/config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_encoder/openvino_model.xml\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_1_0/config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_1_0/diffusion_pytorch_model.fp16.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_1_0/diffusion_pytorch_model.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/unet/openvino_model.bin\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/unet/model.onnx\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/unet/config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/unet/openvino_model.xml\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/unet/diffusion_pytorch_model.fp16.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/unet/diffusion_pytorch_model.safetensors\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/unet/model.onnx_data\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_decoder/openvino_model.bin\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_decoder/model.onnx\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_decoder/config.json\n/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1/vae_decoder/openvino_model.xml\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install invisible_watermark transformers accelerate safetensors google-generativeai\n",
        "!pip install diffusers --upgrade\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "!pip install openai\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T18:23:34.434787Z",
          "iopub.execute_input": "2024-03-02T18:23:34.435154Z",
          "iopub.status.idle": "2024-03-02T18:24:18.305255Z",
          "shell.execute_reply.started": "2024-03-02T18:23:34.435119Z",
          "shell.execute_reply": "2024-03-02T18:24:18.304267Z"
        },
        "trusted": true,
        "id": "FXYc9gzumEkM",
        "outputId": "02298259-5e38-45fd-b049-b3b205933be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting openai\n  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\nDownloading openai-1.13.3-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: openai\nSuccessfully installed openai-1.13.3\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports are written here:\n",
        "\n",
        "import torch\n",
        "import datetime\n",
        "import google.generativeai as genai\n",
        "import openai\n",
        "from diffusers import DiffusionPipeline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T18:24:18.306865Z",
          "iopub.execute_input": "2024-03-02T18:24:18.307257Z",
          "iopub.status.idle": "2024-03-02T18:24:26.736805Z",
          "shell.execute_reply.started": "2024-03-02T18:24:18.307211Z",
          "shell.execute_reply": "2024-03-02T18:24:26.735779Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "81c9a9429e854d60a38ffeecbcc5048a"
          ]
        },
        "id": "sm7YxlxtmEkM",
        "outputId": "fd80e368-a870-4aca-f578-b4df1a8e59b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0it [00:00, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81c9a9429e854d60a38ffeecbcc5048a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the model params\n",
        "generation_config = {\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 32,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "\n",
        "# setting up the safety setting for Openai api\n",
        "safety_settings = [\n",
        "{\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "},\n",
        "{\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "},\n",
        "{\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "},\n",
        "{\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "}\n",
        "]\n",
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "# Define a function for image generation\n",
        "def generate_image(prompt, num_images=1):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"image-alpha-001\",  # You can adjust the engine based on OpenAI's available models\n",
        "        prompt=prompt,\n",
        "        n=num_images,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T18:24:26.738895Z",
          "iopub.execute_input": "2024-03-02T18:24:26.739444Z",
          "iopub.status.idle": "2024-03-02T18:24:26.747356Z",
          "shell.execute_reply.started": "2024-03-02T18:24:26.739404Z",
          "shell.execute_reply": "2024-03-02T18:24:26.746208Z"
        },
        "trusted": true,
        "id": "z2QCDa7mmEkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1\"  # model path\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")  # setting up pipeline\n",
        "\n",
        "pipe.to(\"cuda\")  # use gpu"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T18:24:26.748736Z",
          "iopub.execute_input": "2024-03-02T18:24:26.749072Z",
          "iopub.status.idle": "2024-03-02T18:26:29.92892Z",
          "shell.execute_reply.started": "2024-03-02T18:24:26.749046Z",
          "shell.execute_reply": "2024-03-02T18:26:29.927953Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "07adb689ce9f4cbab369111970f34788"
          ]
        },
        "id": "TBlBQp8jmEkN",
        "outputId": "c52b2e5e-e851-48f4-b49d-978f5eb007f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-02 18:24:28.846168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-02 18:24:28.846330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-02 18:24:29.007091: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07adb689ce9f4cbab369111970f34788"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "StableDiffusionXLPipeline {\n  \"_class_name\": \"StableDiffusionXLPipeline\",\n  \"_diffusers_version\": \"0.26.3\",\n  \"_name_or_path\": \"/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1\",\n  \"feature_extractor\": [\n    null,\n    null\n  ],\n  \"force_zeros_for_empty_prompt\": true,\n  \"image_encoder\": [\n    null,\n    null\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"EulerDiscreteScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"text_encoder_2\": [\n    \"transformers\",\n    \"CLIPTextModelWithProjection\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"tokenizer_2\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calling Openai api for prompt generation\n",
        "def generate_new_prompt() -> dict:\n",
        "    \"\"\"\n",
        "    This function is calling the openai api for creating a perfect prompt for input to stable diffusion model for image generation.\n",
        "    :return: str A perfectly crafted prompt for image generation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt_parts = [\n",
        "\n",
        "            f\"\"\"\n",
        "\n",
        "            Inputs:\n",
        "                Person Description: Description of the person could be man, women, girl or even boy.\n",
        "                Person Looks: Description of look of the person.\n",
        "                Background Description: Description of the background [Studio, Mall, Streets, Beach]\n",
        "                Pose Info: Information about the desired pose and expression of the person. [Standing, Sitting]\n",
        "                Clothing Info: Information about the clothing details.'\n",
        "                Background Enhancements: Suggestions for enhancing the background.'\n",
        "\n",
        "            Create prompt using the above inputs.\n",
        "            The output should be in this format with 50 words:\n",
        "\n",
        "            [Person Description] which looks [Person Looks] in [Background Description] and [Pose Info] with [Cloth Info] with [Background Enhancement].\n",
        "            \"\"\"\n",
        "        ]\n",
        "\n",
        "        # calling the openai api\n",
        "        response = model.generate_content(prompt_parts)\n",
        "\n",
        "        # removing the unwanted symbols from the string and extra characters\n",
        "        text = response.text.replace(\"-\", \"\").replace(\"*\", \"\").replace(\"json\", \"\").replace(\"\", \"\")\n",
        "        text = response.text\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"{e}\"}\n",
        "\n",
        "def get_enhanced_prompt(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    This function is calling the openai api for enhancing and validating the prompt to perfect prompt for input to stable diffusion model for image generation.\n",
        "    :param prompt: Initial prompt from the user.\n",
        "    :return: str A perfectly crafted prompt for image generation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt_parts = [\n",
        "            f\"\"\"\n",
        "            This the original prompt from the user, ehancement the prompt providing the subtle deatils of the person, the background, the cloths and the pose.\n",
        "            Make sure the prompt do have words which affect the photorealism of the image and also dont add too much details of background keep it minimalistic.\n",
        "            The output should be in short, concise and stricly in this format only with stricly lenght under 50 words:\n",
        "            [Person Description] which looks [Person Looks] in [Background Description] and [Pose Info] with [Cloth Info] with [Background Enhancement].\n",
        "            \"\"\"\n",
        "        ]\n",
        "\n",
        "        # calling the openai api\n",
        "        response = model.generate_content(prompt_parts)\n",
        "\n",
        "        # removing the unwanted symbols from the string and extra characters\n",
        "        text = response.text.replace(\"-\", \"\").replace(\"*\", \"\").replace(\"json\", \"\").replace(\"\", \"\")\n",
        "        text = response.text\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"{e}\"}\n",
        "\n",
        "def generate_image(prompt: str):\n",
        "    \"\"\"\n",
        "    Generate image using the prompt provided\n",
        "    \"\"\"\n",
        "    try:\n",
        "        generatedImage = pipe(prompt=prompt).images[0]\n",
        "        generatedImage.save(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".png\")  # saving the image\n",
        "        return generatedImage\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Error\": f\"{e}\"}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T18:27:44.943892Z",
          "iopub.execute_input": "2024-03-02T18:27:44.944706Z",
          "iopub.status.idle": "2024-03-02T18:27:44.95871Z",
          "shell.execute_reply.started": "2024-03-02T18:27:44.944668Z",
          "shell.execute_reply": "2024-03-02T18:27:44.957581Z"
        },
        "trusted": true,
        "id": "-ZpdSiYOmEkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# input from user to get the prompt:\n",
        "\n",
        "# user_prompt = input(\"Enter the prompt for generating the image\")\n",
        "user_prompt = \"women in park\"\n",
        "\n",
        "if len(user_prompt) < 10:\n",
        "    prompt = generate_new_prompt()\n",
        "    print(f\"Prompt : {prompt}\")\n",
        "else:\n",
        "    prompt = get_enhanced_prompt(user_prompt)\n",
        "    print(f\"Prompt: {prompt}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T18:28:02.033032Z",
          "iopub.execute_input": "2024-03-02T18:28:02.033557Z",
          "iopub.status.idle": "2024-03-02T18:28:02.04296Z",
          "shell.execute_reply.started": "2024-03-02T18:28:02.03351Z",
          "shell.execute_reply": "2024-03-02T18:28:02.041674Z"
        },
        "trusted": true,
        "id": "nAdRvaeXmEkN",
        "outputId": "b861da2f-951d-4be5-a4ca-7a0288261dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Prompt: {'Error': \"name 'model' is not defined\"}\nCPU times: user 64 µs, sys: 20 µs, total: 84 µs\nWall time: 91.8 µs\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Generating the image\n",
        "image = generate_image(prompt)\n",
        "image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T18:28:14.187138Z",
          "iopub.execute_input": "2024-03-02T18:28:14.187542Z",
          "iopub.status.idle": "2024-03-02T18:28:14.195036Z",
          "shell.execute_reply.started": "2024-03-02T18:28:14.187513Z",
          "shell.execute_reply": "2024-03-02T18:28:14.194072Z"
        },
        "trusted": true,
        "id": "wCPkpfilmEkN",
        "outputId": "c54be7b4-ca04-46d8-d72f-d9bfe384f514"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "CPU times: user 80 µs, sys: 25 µs, total: 105 µs\nWall time: 109 µs\n",
          "output_type": "stream"
        },
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'Error': \"`prompt` has to be of type `str` or `list` but is <class 'dict'>\"}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wEvityvmEkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}